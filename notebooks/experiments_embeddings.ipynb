{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img: np.ndarray, input_size: tuple[int,int] = (112, 112)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess image for LVFace inference\n",
    "    Args:\n",
    "        img (np.ndarray): Input image in BGR format (from cv2.imread)\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed image tensor\n",
    "    \"\"\"\n",
    "    # Resize image to input size\n",
    "    img_resized = cv2.resize(img, input_size)\n",
    "    # Convert BGR to RGB\n",
    "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "    # Transpose to (C, H, W)\n",
    "    img_transposed = np.transpose(img_rgb, (2, 0, 1))\n",
    "    # Normalize to [-1, 1]\n",
    "    img_normalized = ((img_transposed / 255.0) - 0.5) / 0.5\n",
    "    # Convert to float32 and add batch dimension\n",
    "    img_tensor = img_normalized.astype(np.float32)[np.newaxis, ...]\n",
    "    return img_tensor\n",
    "\n",
    "def create_session(model_path: str):\n",
    "    providers = [\n",
    "        'CUDAExecutionProvider',\n",
    "        'OpenVINOExecutionProvider',\n",
    "        'TensorrtExecutionProvider',\n",
    "        'CPUExecutionProvider',\n",
    "    ]\n",
    "    session = ort.InferenceSession(\n",
    "        model_path,\n",
    "        providers=providers,\n",
    "        provider_options=None\n",
    "    )\n",
    "    print(f\"[INFO] ONNX provider: {session.get_providers()[0]}\")\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"LVFace-S_Glint360K.onnx\"\n",
    "img_list = [\n",
    "            cv2.imread('../example-data/person1_1.jpeg'),\n",
    "            cv2.imread('../example-data/person1_2.jpg'),\n",
    "            cv2.imread('../example-data/person2_1.jpg'),\n",
    "            cv2.imread('../example-data/person2_2.jpg'),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3014f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = [[{'bbox': [117, 79, 309, 372],\n",
    "   'keypoints': [171, 208, 263, 205, 220, 268, 183, 306, 256, 303],\n",
    "   'conf': 0.9086456298828125}],\n",
    " [{'bbox': [207, 48, 398, 349],\n",
    "   'keypoints': [254, 175, 345, 174, 299, 234, 265, 277, 338, 277],\n",
    "   'conf': 0.8973720073699951}],\n",
    " [{'bbox': [186, 191, 545, 691],\n",
    "   'keypoints': [284, 413, 443, 411, 364, 520, 307, 583, 428, 580],\n",
    "   'conf': 0.9050660133361816}],\n",
    " [{'bbox': [83, 34, 285, 325],\n",
    "   'keypoints': [141, 154, 230, 153, 186, 203, 147, 244, 225, 242],\n",
    "   'conf': 0.9086183309555054}],\n",
    " [{'bbox': [122, 103, 372, 452],\n",
    "   'keypoints': [158, 243, 265, 239, 192, 325, 171, 350, 286, 346],\n",
    "   'conf': 0.8931349515914917}],\n",
    " [{'bbox': [495, 217, 806, 644],\n",
    "   'keypoints': [545, 411, 677, 381, 607, 491, 589, 552, 705, 527],\n",
    "   'conf': 0.9142543077468872}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2614cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_112 = np.array([\n",
    "    [38.2946, 51.6963],\n",
    "    [73.5318, 51.5014],\n",
    "    [56.0252, 71.7366],\n",
    "    [41.5493, 92.3655],\n",
    "    [70.7299, 92.2041]\n",
    "], dtype=np.float32)\n",
    "\n",
    "def extract_largest_face_aligned(img: np.ndarray, detections: list[dict], output_size: int = 112) -> np.ndarray | None:\n",
    "    if not detections:\n",
    "        return None\n",
    "\n",
    "    areas = [(d['bbox'][2]-d['bbox'][0]) * (d['bbox'][3]-d['bbox'][1]) for d in detections]\n",
    "    best = detections[np.argmax(areas)]\n",
    "    kps = np.array(best['keypoints']).reshape(5, 2).astype(np.float32)\n",
    "\n",
    "    M, _ = cv2.estimateAffinePartial2D(kps, TEMPLATE_112, method=cv2.LMEDS)\n",
    "    if M is None:\n",
    "        return None\n",
    "\n",
    "    aligned = cv2.warpAffine(\n",
    "        img,\n",
    "        M,\n",
    "        (output_size, output_size),\n",
    "        borderMode=cv2.BORDER_REPLICATE,\n",
    "        flags=cv2.INTER_LINEAR\n",
    "    )\n",
    "    return aligned\n",
    "\n",
    "faces = []\n",
    "for i, img in enumerate(img_list):\n",
    "    face = extract_largest_face_aligned(img, detections[i])\n",
    "    if face is not None:\n",
    "        faces.append(face)\n",
    "    else:\n",
    "        print(f\"No faces on image {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c2b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = []\n",
    "for face in faces:\n",
    "    face_tensor = preprocess_image(face)\n",
    "    tensor_list.append(face_tensor)\n",
    "input = np.vstack(tensor_list)\n",
    "\n",
    "session = create_session(\"../embedding_service/LVFace-S_Glint360K.onnx\")\n",
    "\n",
    "output = session.run(\n",
    "    None,\n",
    "    {\"data\": input}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4eba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "input[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd35b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(feat1: np.ndarray, feat2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two features\n",
    "    \n",
    "    Args:\n",
    "        feat1 (np.ndarray): First feature embedding\n",
    "        feat2 (np.ndarray): Second feature embedding\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity score (range: [-1, 1])\n",
    "    \"\"\"\n",
    "    # Flatten features\n",
    "    feat1_flat = np.ravel(feat1)\n",
    "    feat2_flat = np.ravel(feat2)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    dot_product = np.dot(feat1_flat, feat2_flat)\n",
    "    norm1 = np.linalg.norm(feat1_flat)\n",
    "    norm2 = np.linalg.norm(feat2_flat)\n",
    "    \n",
    "    return dot_product / (norm1 * norm2) if (norm1 > 0 and norm2 > 0) else 0.0\n",
    "\n",
    "print(\"1_1 1_2: \", calculate_similarity(output[0][0], output[0][1]))\n",
    "print(\"1_1 2_1: \", calculate_similarity(output[0][0], output[0][2]))\n",
    "print(\"1_2 2_2: \", calculate_similarity(output[0][1], output[0][3]))\n",
    "print(\"2_1 2_2: \", calculate_similarity(output[0][2], output[0][3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
